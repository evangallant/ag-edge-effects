"""
INPUT DATA
1) a list of s2_blocks, which are the RGB bands for a 20x20 block of pixels (from the 10m resolution S2 satellite imagery) surrounding the target pixel
    These are generated by looking at a set of row, col indices in an S2 image:

    s2_block = data[:, row_start:row_end, col_start:col_end]

    N blocks are appended to an array, which is converted to an np stack:

    s2_blocks = np.stack(s2_blocks)

2) a list of corresponding land cover classes from the USFS dataset at the target pixel's location
    Simple list of integers

OUTPUT
1) An integer representing the class that the model thinks applies to the target pixel
"""
import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from pathlib import Path

model_dir = os.path.dirname(os.path.abspath('__file__'))
src_dir = Path(model_dir).parent
root_dir = Path(src_dir).parent
if str(root_dir) not in sys.path:
    sys.path.append(str(root_dir))
from src.data.CNN.cnn_data_generator import generate_training_samples, generate_roi_list


# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# USFS Land Cover Class Mapping 
class_names = {
    1: "Trees",
    2: "Tall Shrubs & Trees Mix (SEAK Only)", 
    3: "Shrubs & Trees Mix",
    4: "Grass/Forb/Herb & Trees Mix",
    5: "Barren & Trees Mix",
    6: "Tall Shrubs (SEAK Only)",
    7: "Shrubs",
    8: "Grass/Forb/Herb & Shrubs Mix",
    9: "Barren & Shrubs Mix",
    10: "Grass/Forb/Herb",
    11: "Barren & Grass/Forb/Herb Mix",
    12: "Barren or Impervious",
    13: "Snow or Ice",
    14: "Water",
    15: "Non-Processing Area Mask"
}

# Custom Dataset Class
class LandCoverDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx] - 1
        
        # Convert to PyTorch tensor
        image = torch.tensor(image, dtype=torch.float32)
        
        # Apply transformations if any
        if self.transform:
            image = self.transform(image)
            
        return image, label

class SpatialAttention(nn.Module):
    """
    Spatial attention module that emphasizes the center of the input feature maps.
    Combines a learned attention with a center-biased prior.
    """
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        # Conv layer to generate attention map
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # Generate center-biased prior (Gaussian-like)
        batch, channels, height, width = x.size()
        
        # Create coordinate grids
        y_grid, x_grid = torch.meshgrid(
            torch.linspace(-1, 1, height, device=x.device),
            torch.linspace(-1, 1, width, device=x.device),
            indexing='ij'
        )
        
        # Calculate distance from center (0,0)
        distance_squared = x_grid**2 + y_grid**2
        
        # Convert to Gaussian with sigma=0.5 (controls how quickly attention drops off)
        center_prior = torch.exp(-distance_squared / 0.5)
        center_prior = center_prior.unsqueeze(0).unsqueeze(0)
        
        # Generate feature-derived attention
        # Channel pooling: avg_pool captures general activations, max_pool captures salient features
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        
        # Concatenate pooled features
        pooled = torch.cat([avg_pool, max_pool], dim=1)
        
        # Generate attention map from pooled features
        learned_attention = self.sigmoid(self.conv(pooled))
        
        # Combine learned attention with center prior
        # This creates an attention map that considers both feature importance and center bias
        combined_attention = learned_attention * center_prior
        
        # Visualize the attention map (for debugging)
        # plt.imshow(combined_attention[0, 0].cpu().detach().numpy())
        # plt.colorbar()
        # plt.title("Spatial Attention Map")
        # plt.show()
        
        return combined_attention

class LandCoverCNN(nn.Module):
    def __init__(self, num_classes, block_size=15):
        super(LandCoverCNN, self).__init__()
        
        # Input: 4×block_size×block_size --> RGB + NIR bands
        # We'll add NDVI as a 5th channel in the forward pass
        self.conv1 = nn.Conv2d(5, 32, kernel_size=3, padding=1)  # 5 channels (RGB+NIR+NDVI)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(2)
        
        # Attention mechanisms after first conv block
        self.spectral_attention1 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),    # Averages all values for the feature map into a single 1x1 value
            nn.Conv2d(32, 8, 1),        # Channel reduction (32 --> 8) to force the model to identify only the important relationships
            nn.ReLU(),                  # Introduce non-linearity
            nn.Conv2d(8, 32, 1),        # Upscale back to 32 channels
            nn.Sigmoid()                # Normalize the values to a 0-1 range, and produce a soft attention mask
        )
        self.spatial_attention1 = SpatialAttention(kernel_size=7)
        
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(2)
        
        # Attention mechanisms after second conv block
        self.spectral_attention2 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(64, 16, 1),
            nn.ReLU(),
            nn.Conv2d(16, 64, 1),
            nn.Sigmoid()
        )
        self.spatial_attention2 = SpatialAttention(kernel_size=5)
        
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(2)
        
        # Attention mechanisms after third conv block
        self.spectral_attention3 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(128, 32, 1),
            nn.ReLU(),
            nn.Conv2d(32, 128, 1),
            nn.Sigmoid()
        )
        self.spatial_attention3 = SpatialAttention(kernel_size=3)
        
        # Calculate feature dimensions after pooling
        feature_size = block_size // 8
        if block_size % 8 != 0:
            feature_size = feature_size + 1
            
        self.feature_size = feature_size
        flattened_size = 128 * feature_size * feature_size
        
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(flattened_size, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, num_classes)
        
        print(f"Block size: {block_size}×{block_size}")
        print(f"Feature map after pooling: 128 × {feature_size} × {feature_size}")
        print(f"Flattened features: {flattened_size}")
        
    def calculate_ndvi(self, x):
        """Calculate NDVI from RGB+NIR input"""
        # NIR is channel 3, Red is channel 2
        nir = x[:, 3:4, :, :]  # Extract NIR band and keep dimensions
        red = x[:, 2:3, :, :]  # Extract Red band and keep dimensions
        
        # Calculate NDVI: (NIR - Red) / (NIR + Red)
        # Adding small epsilon to avoid division by zero
        return (nir - red) / (nir + red + 1e-8)
        
    def forward(self, x):
        # Calculate NDVI and add as a 5th channel
        ndvi = self.calculate_ndvi(x)
        x = torch.cat([x, ndvi], dim=1)  # Concatenate along channel dimension
        
        # First convolutional block
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        
        # Apply spectral attention (emphasize important channels)
        spectral_weights1 = self.spectral_attention1(x)
        x = x * spectral_weights1
        
        # Apply spatial attention (emphasize center region)
        spatial_weights1 = self.spatial_attention1(x)
        x = x * spatial_weights1
        
        # Continue with pooling
        x = self.pool1(x)
        
        # Second convolutional block with attention
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        
        spectral_weights2 = self.spectral_attention2(x)
        x = x * spectral_weights2
        
        spatial_weights2 = self.spatial_attention2(x)
        x = x * spatial_weights2
        
        x = self.pool2(x)
        
        # Third convolutional block
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        
        spectral_weights3 = self.spectral_attention3(x)
        x = x * spectral_weights3
        
        spatial_weights3 = self.spatial_attention3(x)
        x = x * spatial_weights3
        
        x = self.pool3(x)
        
        # Flatten and fully connected layers
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
    
# Main training function
def train_land_cover_model(s2_blocks, classes, batch_size=32, epochs=50, learning_rate=0.001, block_size=15):
    """
    Train a CNN model to classify land cover types from Sentinel-2 image blocks
    
    Parameters:
    -----------
    s2_blocks : numpy.ndarray
        Array of image blocks with shape (n_samples, 3, block_size, block_size)
    classes : numpy.ndarray
        Array of class labels
    batch_size : int
        Batch size for training
    epochs : int
        Number of training epochs
    learning_rate : float
        Learning rate for optimizer
    block_size : int
        Size of the input image blocks
        
    Returns:
    --------
    model : LandCoverCNN
        Trained CNN model
    history : dict
        Training history (loss and accuracy)
    """
    # 1. Data Preprocessing
    print("Preprocessing data...")
    
    # Normalize the data (assuming typical Sentinel-2 surface reflectance values 0-10000)
    s2_blocks = s2_blocks.astype(np.float32) / 10000.0
    
    # Check if blocks are already in the right format (C, H, W)
    if len(s2_blocks.shape) == 4:
        # Determine format by analyzing shapes
        n_samples = s2_blocks.shape[0]
        
        # Possible formats:
        # (N, C, H, W) - PyTorch format, channels are dimension 1
        # (N, H, W, C) - TensorFlow format, channels are dimension 3
        
        # Check which dimension is likely the channel dimension (smallest of the three)
        potential_channel_dims = [s2_blocks.shape[1], s2_blocks.shape[2], s2_blocks.shape[3]]
        potential_channel_idx = np.argmin(potential_channel_dims) + 1  # +1 because we skip dimension 0
        
        if potential_channel_idx == 3 and potential_channel_dims[2] <= 10:  # Assume max 10 bands
            # Format is likely (N, H, W, C)
            print(f"Detected (N, H, W, C) format with {s2_blocks.shape[3]} channels, transposing...")
            s2_blocks = np.transpose(s2_blocks, (0, 3, 1, 2))

        # Print final format
        print(f"Final data shape: {s2_blocks.shape} (N, C, H, W)")
    
    # Check data range
    print(f"Data range: {s2_blocks.min()} to {s2_blocks.max()}")
    
    # Split data into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(
        s2_blocks, classes, test_size=0.2, stratify=classes, random_state=42
    )
    
    print(f"Training samples: {len(X_train)}, Validation samples: {len(X_val)}")
    
    # 2. Create datasets and dataloaders
    # Custom transforms for data augmentation (optional)
    train_transform = transforms.Compose([
        # These transforms work on tensors
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
    ])
    
    train_dataset = LandCoverDataset(X_train, y_train, transform=train_transform)
    val_dataset = LandCoverDataset(X_val, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    # 3. Set up the model
    # Get number of unique classes
    num_classes = len(np.unique(classes))
    print(f"Number of classes: {num_classes}")
    
    # Initialize model, loss, and optimizer
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    total_possible_classes = 15
    model = LandCoverCNN(total_possible_classes, block_size=block_size).to(device)

    # Create weights for all possible classes (1-15)
    class_counts = np.bincount(y_train, minlength=total_possible_classes+1)[1:]  # Skip index 0 since classes start at 1
    
    # Ensure there are no zeros in class_counts (which would cause division by zero)
    min_count = 1  # Minimum count to avoid division by zero
    class_counts = np.maximum(class_counts, min_count)
    
    # Create class weights
    class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)
    class_weights = class_weights / class_weights.sum() * len(class_counts)
    class_weights = class_weights.to(device)
    
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # Add learning rate scheduler for better convergence
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )
    
    # 4. Training Loop
    print("Starting training...")
    
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]"):
            images, labels = images.to(device), labels.to(device)
            
            # Zero the gradients
            optimizer.zero_grad()
            
            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # Backward pass and optimize
            loss.backward()
            optimizer.step()
            
            # Track statistics
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
        
        train_loss = train_loss / len(train_loader)
        train_acc = 100 * correct_train / total_train
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        correct_val = 0
        total_val = 0
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]"):
                images, labels = images.to(device), labels.to(device)
                
                # Forward pass
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                # Track statistics
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()
        
        val_loss = val_loss / len(val_loader)
        val_acc = 100 * correct_val / total_val
        
        # Update learning rate
        scheduler.step(val_loss)
        
        # Save history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        print(f"Epoch {epoch+1}/{epochs} - "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        
        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save the best model
            torch.save({
                'model_state_dict': model.state_dict(),
                'block_size': block_size,
                'num_classes': num_classes,
                # Any other parameters you might need
            }, 'rgb_nir_80x80_CNN_block_model.pth')  # TODO: UPDATE MODEL PATH NAME
            print("Saved best model")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping after {epoch+1} epochs")
                break
    
    # 5. Load the best model for evaluation
    # Load the checkpoint
    checkpoint = torch.load('rgb_nir_80x80_CNN_block_model.pth')  # TODO: UPDATE MODEL PATH NAME

    # Initialize model with the saved parameters
    model = LandCoverCNN(
        num_classes=checkpoint['num_classes'],
        block_size=checkpoint['block_size']
    )

    # Load the state dictionary
    model.load_state_dict(checkpoint['model_state_dict'])
    
    return model, history

# Evaluation function
def evaluate_model(model, s2_blocks, classes):
    """
    Evaluate the trained model on test data
    
    Parameters:
    -----------
    model : LandCoverCNN
        Trained model
    s2_blocks : numpy.ndarray
        Test image blocks
    classes : numpy.ndarray
        True class labels
        
    Returns:
    --------
    y_true : numpy.ndarray
        True labels
    y_pred : numpy.ndarray
        Predicted labels
    """
    # Normalize the data
    s2_blocks = s2_blocks.astype(np.float32) / 10000.0
    
    # Create dataset and dataloader
    test_dataset = LandCoverDataset(s2_blocks, classes)
    test_loader = DataLoader(test_dataset, batch_size=32)
    
    device = next(model.parameters()).device
    model.eval()
    
    all_preds = []
    all_true = []
    
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Evaluating"):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_true.extend(labels.cpu().numpy())
    
    return np.array(all_true), np.array(all_preds)

# Visualization functions
def plot_training_history(history):
    """Plot training and validation loss/accuracy curves"""
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss Curves')
    
    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Train Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.title('Accuracy Curves')
    
    plt.tight_layout()
    plt.savefig('rgb_nir_training_history.png') # TODO: UPDATE FIG NAME
    plt.show()

def plot_confusion_matrix(y_true, y_pred, class_names_dict):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    
    # Normalize by row (true labels)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Get unique classes in the data
    unique_classes = np.unique(np.concatenate([y_true, y_pred]))
    class_labels = [class_names_dict.get(int(cls), f"Class {cls}") for cls in unique_classes]
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Normalized Confusion Matrix')
    plt.tight_layout()
    plt.savefig('rgb_nir_confusion_matrix.png') # TODO: UPDATE FIG NAME
    plt.show()
    
    # Also print classification report
    print("\nClassification Report:")
    report = classification_report(y_true, y_pred, 
                                  target_names=[class_names_dict.get(int(cls), f"Class {cls}") 
                                              for cls in unique_classes])
    print(report)

# Main execution block
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(script_dir, '..', '..', '..'))
    s2_data_folder = os.path.join(project_root, 'data', 'raw', 'sentinel2_imagery')
    lc_data_folder = os.path.join(project_root, 'data', 'raw', 'USFS_land_cover')

    checkpoint = torch.load('rgb_nir_80x80_CNN_block_model.pth')

    # Initialize model with the saved parameters
    model = LandCoverCNN(
        num_classes=15,
        block_size=checkpoint['block_size']
    )

    # Load the state dictionary
    model.load_state_dict(checkpoint['model_state_dict'])

    roi_names = [
        "kit_carson",
        "cripple_creek",
        "montrose",
        "cortez",
        "durango",
        "lizard_head",
        "ridgway",
        "uncompahgre",
        "yuma",
        "centennial",
        "gunnison",
        "powderhorn",
        "lake_city",
        "monte_vista",
        "boulder", 
        "creede",
        "deer_tail", 
        "fort_collins",
        "fort_morgan", 
        "hunter-fryingpan",
        "lamar",
        "leadville",
        "mt_harvard",
        "pritchett",
        "stratton",
        "trinidad"
    ]

    roi_list = generate_roi_list(roi_names, s2_data_folder, lc_data_folder)
    block_size = 80

    s2_blocks, classes, s2_block_metadata, utm_coords, albers_coords, lat_lon_coords = generate_training_samples(roi_list, block_size, 150000)
    
    # print(f"Loaded {len(s2_blocks)} samples with {len(np.unique(classes))} unique classes")
    # print(f"Block shape: {s2_blocks.shape}")
    
    # # Check class distribution
    # unique_classes, class_counts = np.unique(classes, return_counts=True)
    # print("\nClass distribution:")
    # for cls, count in zip(unique_classes, class_counts):
    #     if count == 1:
    #         print(f"Class {cls} found with {count} (should be 1) samples - exiting early")
    #         raise ValueError("Class with only 1 sample found")
        
    #     print(f"Class {cls} ({class_names.get(cls, 'Unknown')}): {count} samples")
    
    # # Train model
    # model, history = train_land_cover_model(
    #     s2_blocks, 
    #     classes,
    #     batch_size=32,
    #     epochs=50,
    #     learning_rate=0.001,
    #     block_size=block_size
    # )
    
    # # Visualize training progress
    # plot_training_history(history)


    X_train, X_test, y_train, y_test = train_test_split(s2_blocks, classes, test_size=0.2, random_state=42)

    # Pass the test data to evaluate_model
    y_true, y_pred = evaluate_model(model, X_test, y_test)
    
    # Plot confusion matrix
    plot_confusion_matrix(y_true, y_pred, class_names)
    
    print("Training and evaluation complete!")